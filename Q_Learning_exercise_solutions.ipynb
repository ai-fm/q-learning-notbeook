{"cells":[{"cell_type":"markdown","source":["# Introduction to Q-Learning\n","---\n","\n","In this Jupyter Notebook you will learn the basics of Q-Learning.\n","\n","### What is Jupyter Notebook?\n","---\n","- Jupyter Notebooks can run (Python) code interactively.\n","- There are cells (= code boxes) in which you can write code.\n","- Once you are finished writing your code you can select any cell and click the play button at the top left to run the cell.\n","- Cells can be run multiple times and in arbitrary order.\n","\n","\n","### Prerequisites\n","---\n","\n","**Option 1**: You can run the code in this Jupyter Notebook on your own laptop. But this requires you to install Python and a lot of dependencies.\n","\n","\n","**Much easier:** You can use Google Colab, which runs all the code for you in the cloud and has Python already installed for you. The only downside is that it requires a Google Account.\n","\n","\n","### Credits\n","---\n","- [Huggingface Deep Reinforcement Learning Course](https://huggingface.co/learn/deep-rl-course/unit0/introduction)\n","- [Gymnasium](https://gymnasium.farama.org/)\n"],"metadata":{"id":"fyTo5yHvlk1E"}},{"cell_type":"markdown","metadata":{"id":"njb_ProuHiOe"},"source":["# Unit 2: Q-Learning with FrozenLake-v1 ‚õÑ and Taxi-v3 üöï\n","\n","\n","In this notebook, **you'll code your first Reinforcement Learning agent from scratch** to play FrozenLake ‚ùÑÔ∏è using Q-Learning.\n","\n","‚¨áÔ∏è Here is an example of what **you will achieve.** ‚¨áÔ∏è\n"]},{"cell_type":"markdown","metadata":{"id":"vRU_vXBrl1Jx"},"source":["<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/envs.gif\" alt=\"Environments\"/>"]},{"cell_type":"markdown","source":["###üéÆ Environments:\n","\n","- [FrozenLake-v1](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n","- [Taxi-v3](https://gymnasium.farama.org/environments/toy_text/taxi/)"],"metadata":{"id":"DPTBOv9HYLZ2"}},{"cell_type":"markdown","metadata":{"id":"4i6tjI2tHQ8j"},"source":["## Objectives of this notebook üèÜ\n","\n","At the end of the notebook, you will:\n","\n","- Be able to use **Gymnasium**, the environment library.\n","- Be able to code a Q-Learning agent from scratch.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"f2ONOODsyrMU"},"source":["## A small recap of Q-Learning"]},{"cell_type":"markdown","metadata":{"id":"V68VveLacfxJ"},"source":["*Q-Learning* **is the Reinforcement Learning algorithm that**:\n","\n","- Trains *Q-Function*, an **action-value function** that encoded, in internal memory, by a *Q-table* **that contains all the state-action pair values.**\n","\n","- Given a state and action, our Q-Function **will search the Q-table for the corresponding value.**\n","\n","- In simpler terms: For a given state the Q-table tells us how good future states are that we might end up in when taking an action. The Q-value is a numerical number. Higher values represent preferrable states.\n","    \n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg\" alt=\"Q function\"  width=\"100%\"/>\n","\n","- When the training is done,**we have an optimal Q-Function, so an optimal Q-Table.**\n","    \n","- And if we **have an optimal Q-function**, we\n","have an optimal policy œÄ, since we **know for, each state, the best action to take.**\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" alt=\"Link value policy\"  width=\"100%\"/>\n","\n","\n","But, in the beginning (before training),¬†our **Q-Table is useless since it gives wrong values for each state-action pair¬†(most of the time we initialize the Q-Table to 0 values)**. But, as we‚Äôll¬†explore the environment and update our Q-Table it will give us better and better approximations\n","\n","This is the Q-Learning pseudocode:\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"]},{"cell_type":"markdown","source":["# Let's code our first Reinforcement Learning algorithm üöÄ"],"metadata":{"id":"HEtx8Y8MqKfH"}},{"cell_type":"markdown","source":["## Install dependencies and create a virtual display üîΩ\n","\n","In the notebook, we'll need to generate a replay video. To do so, with Colab, **we need to have a virtual screen to render the environment** (and thus record the frames).\n","\n","Hence the following cell will install the libraries and create and run a virtual screen üñ•\n","\n","We‚Äôll install multiple ones:\n","\n","- `gymnasium`: Contains the FrozenLake-v1 ‚õÑ and Taxi-v3 üöï virtual environments.\n","- `pygame`: Used for the visualization of FrozenLake-v1 and Taxi-v3.\n","- `numpy`: Used for handling our Q-table.\n"],"metadata":{"id":"4gpxC1_kqUYe"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9XaULfDZDvrC"},"outputs":[],"source":["!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt"]},{"cell_type":"code","source":["!sudo apt-get update\n","!sudo apt-get install -y python3-opengl\n","!apt install ffmpeg xvfb\n","!pip3 install pyvirtualdisplay"],"metadata":{"id":"n71uTX7qqzz2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To make sure the new installed libraries are used, **sometimes it's required to restart the notebook runtime**. The next cell will force the **runtime to crash, so you'll need to connect again and run the code starting from here**. Thanks to this trick, **we will be able to run our virtual screen.**"],"metadata":{"id":"K6XC13pTfFiD"}},{"cell_type":"code","source":["import os\n","os.kill(os.getpid(), 9)"],"metadata":{"id":"3kuZbWAkfHdg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Virtual display\n","from pyvirtualdisplay import Display\n","\n","virtual_display = Display(visible=0, size=(1400, 900))\n","virtual_display.start()"],"metadata":{"id":"DaY1N4dBrabi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W-7f-Swax_9x"},"source":["## Import the packages üì¶\n","\n","In addition to the installed libraries, we also use:\n","\n","- `random`: To generate random numbers\n","- `imageio`: To generate a replay video."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VcNvOAQlysBJ"},"outputs":[],"source":["import numpy as np\n","import gymnasium as gym\n","import random\n","import imageio\n","import os\n","import tqdm\n","\n","import pickle5 as pickle\n","from tqdm.notebook import tqdm\n","\n","from IPython.display import Image"]},{"cell_type":"markdown","metadata":{"id":"xp4-bXKIy1mQ"},"source":["We're now ready to code our Q-Learning algorithm üî•"]},{"cell_type":"markdown","metadata":{"id":"xya49aNJWVvv"},"source":["# Part 1: Frozen Lake ‚õÑ (non slippery version)"]},{"cell_type":"markdown","metadata":{"id":"NAvihuHdy9tw"},"source":["## Create and understand [FrozenLake environment ‚õÑ]((https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n","---\n","\n","üí° A good habit when you start to use an environment is to check its documentation\n","\n","üëâ https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n","\n","---\n","\n","We're going to train our Q-Learning agent **to navigate from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H)**.\n","\n","We can have two sizes of environment:\n","\n","- `map_name=\"4x4\"`: a 4x4 grid version\n","- `map_name=\"8x8\"`: a 8x8 grid version\n","\n","\n","The environment has two modes:\n","\n","- `is_slippery=False`: The agent always moves **in the intended direction** due to the non-slippery nature of the frozen lake (deterministic).\n","- `is_slippery=True`: The agent **may not always move in the intended direction** due to the slippery nature of the frozen lake (stochastic)."]},{"cell_type":"markdown","metadata":{"id":"UaW_LHfS0PY2"},"source":["For now let's keep it simple with the 4x4 map and non-slippery.\n","We add a parameter called `render_mode` that specifies how the environment should be visualised. In our case because we **want to record a video of the environment at the end, we need to set `render_mode` to rgb_array**.\n","\n","As [explained in the documentation](https://gymnasium.farama.org/api/env/#gymnasium.Env.render) `rgb_array`: Return a single frame representing the current state of the environment. A frame is a np.ndarray with shape (x, y, 3) representing RGB values for an x-by-y pixel image.\n","\n","`desc` can be `None`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IzJnb8O3y8up"},"outputs":[],"source":["# Create the FrozenLake-v1 environment using 4x4 map and non-slippery version and render_mode=\"rgb_array\"\n","env = gym.make(...) # TODO use the correct parameters"]},{"cell_type":"markdown","metadata":{"id":"Ji_UrI5l2zzn"},"source":["### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jNxUbPMP0akP"},"outputs":[],"source":["env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")"]},{"cell_type":"markdown","metadata":{"id":"KASNViqL4tZn"},"source":["You can create your own custom grid like this:\n","\n","```python\n","desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n","gym.make('FrozenLake-v1', desc=desc, is_slippery=True)\n","```\n","\n","but we'll use the default environment for now."]},{"cell_type":"markdown","metadata":{"id":"SXbTfdeJ1Xi9"},"source":["### Let's see what the Environment looks like:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZNPG0g_UGCfh"},"outputs":[],"source":["# We create our environment with gym.make(\"<name_of_the_environment>\")- `is_slippery=False`: The agent always moves in the intended direction due to the non-slippery nature of the frozen lake (deterministic).\n","print(\"_____OBSERVATION SPACE_____ \\n\")\n","print(\"Observation Space\", env.observation_space)\n","print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"]},{"cell_type":"markdown","metadata":{"id":"2MXc15qFE0M9"},"source":["We see with `Observation Space Shape Discrete(16)` that the observation is an integer representing the **agent‚Äôs current position as current_row * ncols + current_col (where both the row and col start at 0)**.\n","\n","For example, the goal position in the 4x4 map can be calculated as follows: 3 * 4 + 3 = 15. The number of possible observations is dependent on the size of the map. **For example, the 4x4 map has 16 possible observations.**\n","\n","\n","For instance, this is what state = 0 looks like:\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/frozenlake.png\" alt=\"FrozenLake\">"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"We5WqOBGLoSm"},"outputs":[],"source":["print(\"\\n _____ACTION SPACE_____ \\n\")\n","print(\"Action Space Shape\", env.action_space.n)\n","print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"]},{"cell_type":"markdown","metadata":{"id":"MyxXwkI2Magx"},"source":["The action space (the set of possible actions the agent can take) is discrete with 4 actions available üéÆ:\n","- 0: GO LEFT\n","- 1: GO DOWN\n","- 2: GO RIGHT\n","- 3: GO UP\n","\n","Reward function üí∞:\n","- Reach goal: +1\n","- Reach hole: 0\n","- Reach frozen: 0"]},{"cell_type":"markdown","metadata":{"id":"1pFhWblk3Awr"},"source":["## Create and Initialize the Q-table üóÑÔ∏è\n","\n","(üëÄ Step 1 of the pseudocode)\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n","\n","\n","It's time to initialize our Q-table! To know how many rows (states) and columns (actions) to use, we need to know the action and observation space. We already know their values from before, but we'll want to obtain them programmatically so that our algorithm generalizes for different environments. Gym provides us a way to do that: `env.action_space.n` and `env.observation_space.n`\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y3ZCdluj3k0l"},"outputs":[],"source":["state_space =\n","print(\"There are \", state_space, \" possible states\")\n","\n","action_space =\n","print(\"There are \", action_space, \" possible actions\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rCddoOXM3UQH"},"outputs":[],"source":["# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros. np.zeros needs a tuple (a,b)\n","def initialize_q_table(state_space, action_space):\n","  Qtable =\n","  return Qtable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9YfvrqRt3jdR"},"outputs":[],"source":["Qtable_frozenlake = initialize_q_table(state_space, action_space)"]},{"cell_type":"markdown","metadata":{"id":"67OdoKL63eDD"},"source":["### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HuTKv3th3ohG"},"outputs":[],"source":["state_space = env.observation_space.n\n","print(\"There are \", state_space, \" possible states\")\n","\n","action_space = env.action_space.n\n","print(\"There are \", action_space, \" possible actions\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lnrb_nX33fJo"},"outputs":[],"source":["# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros\n","def initialize_q_table(state_space, action_space):\n","  Qtable = np.zeros((state_space, action_space))\n","  return Qtable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y0WlgkVO3Jf9"},"outputs":[],"source":["Qtable_frozenlake = initialize_q_table(state_space, action_space)"]},{"cell_type":"markdown","metadata":{"id":"Atll4Z774gri"},"source":["## Define the greedy policy ü§ñ\n","\n","Remember we have two policies since Q-Learning is an **off-policy** algorithm. This means we're using a **different policy for acting and updating the value function**.\n","\n","- Epsilon-greedy policy (acting policy)\n","- Greedy-policy (updating policy)\n","\n","The greedy policy will also be the final policy we'll have when the Q-learning agent completes training. The greedy policy is used to select an action using the Q-table.\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E3SCLmLX5bWG"},"outputs":[],"source":["def greedy_policy(Qtable, state):\n","  # Exploitation: take the action with the highest state, action value\n","  # (argmax could be useful here)\n","  action =\n","\n","  return action"]},{"cell_type":"markdown","metadata":{"id":"B2_-8b8z5k54"},"source":["#### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"se2OzWGW5kYJ"},"outputs":[],"source":["def greedy_policy(Qtable, state):\n","  # Exploitation: take the action with the highest state, action value\n","  action = np.argmax(Qtable[state, :])\n","\n","  return action"]},{"cell_type":"markdown","metadata":{"id":"flILKhBU3yZ7"},"source":["##Define the epsilon-greedy policy ü§ñ\n","\n","Epsilon-greedy is the training policy that handles the exploration/exploitation trade-off.\n","\n","The idea with epsilon-greedy:\n","\n","- With *probability 1‚Ää-‚Ää…õ* : **we do exploitation** (i.e. our agent selects the action with the highest state-action pair value).\n","\n","- With *probability …õ*: we do **exploration** (trying a random action).\n","\n","As the training continues, we progressively **reduce the epsilon value since we will need less and less exploration and more exploitation.**\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Bj7x3in3_Pq"},"outputs":[],"source":["def epsilon_greedy_policy(Qtable, state, epsilon):\n","  # Randomly generate a number between 0 and 1\n","  random_num =\n","  # if random_num > greater than epsilon --> exploitation\n","  if random_num > epsilon:\n","    # Take the action with the highest value given a state\n","    # remember to reuse code and functions\n","    action =\n","  # else --> exploration\n","  else:\n","    # Take a random action\n","    action =\n","\n","  return action"]},{"cell_type":"markdown","metadata":{"id":"8R5ej1fS4P2V"},"source":["#### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cYxHuckr4LiG"},"outputs":[],"source":["def epsilon_greedy_policy(Qtable, state, epsilon):\n","  # Randomly generate a number between 0 and 1\n","  random_num = random.uniform(0,1)  # or random.random()\n","  # if random_num > greater than epsilon --> exploitation\n","  if random_num > epsilon:\n","    # Take the action with the highest value given a state\n","    # np.argmax can be useful here\n","    action = greedy_policy(Qtable, state)\n","  # else --> exploration\n","  else:\n","    action = env.action_space.sample()\n","\n","  return action"]},{"cell_type":"markdown","metadata":{"id":"hW80DealcRtu"},"source":["## Define the hyperparameters ‚öôÔ∏è\n","\n","The exploration related hyperparamters are some of the most important ones.\n","\n","- Hyperparameters are variables that are not learned or improved during training. They have to be defined before we run our training experiment and they can heavily influence the quality of our results.\n","- We need to make sure that our agent **explores enough of the state space** to learn a good value approximation. So, in the beginning the agent should be explorative and over time it should prefer using its acquired knowledge to best achieve its goals. To do that, we need to have progressive decay of the epsilon.\n","- If you decrease epsilon too fast (too high decay_rate), **you take the risk that your agent will be stuck**, since your agent didn't explore enough of the state space, did not experience enough ways to solve the problem and hence can't solve the problem."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y1tWn0tycWZ1"},"outputs":[],"source":["# Training parameters\n","n_training_episodes = 10000  # Total training episodes\n","learning_rate = 0.7          # Learning rate\n","\n","# Evaluation parameters\n","n_eval_episodes = 100        # Total number of test episodes\n","\n","# Environment parameters\n","env_id = \"FrozenLake-v1\"     # Name of the environment\n","max_steps = 99               # Max steps per episode\n","gamma = 0.95                 # Discounting rate\n","eval_seed = []               # The evaluation seed of the environment\n","\n","# Exploration parameters\n","max_epsilon = 1.0             # Exploration probability at start\n","min_epsilon = 0.05            # Minimum exploration probability\n","decay_rate = 0.0005            # Exponential decay rate for exploration prob"]},{"cell_type":"markdown","metadata":{"id":"cDb7Tdx8atfL"},"source":["## Create the training loop method\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n","\n","The training loop goes like this:\n","\n","```\n","For episode in the total of training episodes:\n","\n","Reduce epsilon (since we need less and less exploration)\n","Reset the environment\n","\n","  For step in max timesteps:    \n","    Choose the action At using epsilon greedy policy\n","    Take the action (a) and observe the outcome state(s') and reward (r)\n","    Update the Q-value Q(s,a) using Bellman equation Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n","    If done, finish the episode\n","    Our next state is the new state\n","```"]},{"cell_type":"markdown","source":["**Task:** Fill in the code where it is missing below!"],"metadata":{"id":"4_QbAGwArwK6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"paOynXy3aoJW"},"outputs":[],"source":["def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable, gamma, learning_rate):\n","  for episode in tqdm(range(n_training_episodes)):\n","    # Reduce epsilon (because we need less and less exploration)\n","    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n","    # Reset the environment\n","    state, info = env.reset()\n","    step = 0\n","    terminated = False\n","    truncated = False\n","\n","    # repeat\n","    for step in range(max_steps):\n","      # Choose the action At using epsilon greedy policy\n","      action =\n","\n","      # Take action At and observe Rt+1 and St+1\n","      # Take the action (a) and observe the outcome state(s') and reward (r)\n","      # Note: The action is performed by the environment\n","      new_state, reward, terminated, truncated, info =\n","\n","      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n","\n","\n","      # If terminated or truncated finish the episode\n","      if terminated or truncated:\n","        break\n","\n","      # Our next state is the new state\n","      state = new_state\n","  return Qtable"]},{"cell_type":"markdown","metadata":{"id":"Pnpk2ePoem3r"},"source":["#### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IyZaYbUAeolw"},"outputs":[],"source":["def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable, gamma, learning_rate):\n","  for episode in tqdm(range(n_training_episodes)):\n","    # Reduce epsilon (because we need less and less exploration)\n","    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n","    # Reset the environment\n","    state, info = env.reset()\n","    step = 0\n","    terminated = False\n","    truncated = False\n","\n","    # repeat\n","    for step in range(max_steps):\n","      # Choose the action At using epsilon greedy policy\n","      action = epsilon_greedy_policy(Qtable, state, epsilon)\n","\n","      # Take action At and observe Rt+1 and St+1\n","      # Take the action (a) and observe the outcome state(s') and reward (r)\n","      # Note: The action is performed by the environment\n","      new_state, reward, terminated, truncated, info = env.step(action)\n","\n","      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n","      Q = Qtable[state, action]                    # Value of current state\n","      Q_max_new = np.max(Qtable[new_state, :])     # Value of best future state\n","      Qtable[state, action] = Q + learning_rate * (reward + gamma * Q_max_new - Q)\n","\n","      # If terminated or truncated finish the episode\n","      if terminated or truncated:\n","        break\n","\n","      # Our next state is the new state\n","      state = new_state\n","  return Qtable"]},{"cell_type":"markdown","metadata":{"id":"WLwKQ4tUdhGI"},"source":["## Train the Q-Learning agent üèÉ"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DPBxfjJdTCOH"},"outputs":[],"source":["Qtable_frozenlake =  # Your code here"]},{"cell_type":"markdown","metadata":{"id":"IW7B2Xb9nAKL"},"source":["#### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TBot7iRunAKP"},"outputs":[],"source":["Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake, gamma, learning_rate)"]},{"cell_type":"markdown","metadata":{"id":"yVeEhUCrc30L"},"source":["## Let's see what our Q-Learning table looks like now üëÄ"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nmfchsTITw4q"},"outputs":[],"source":["Qtable_frozenlake"]},{"cell_type":"markdown","metadata":{"id":"pUrWkxsHccXD"},"source":["## The evaluation method üìù\n","\n","- We defined the evaluation method that we're going to use to test our Q-Learning agent."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jNl0_JO2cbkm"},"outputs":[],"source":["################################\n","#  DO NOT CHANGE THIS FUNCTION #\n","################################\n","\n","def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n","  \"\"\"\n","  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n","  :param env: The evaluation environment\n","  :param max_steps: Maximum number of steps per episode\n","  :param n_eval_episodes: Number of episode to evaluate the agent\n","  :param Q: The Q-table\n","  :param seed: The evaluation seed array (for taxi-v3)\n","  \"\"\"\n","  episode_rewards = []\n","  for episode in tqdm(range(n_eval_episodes)):\n","    if seed:\n","      state, info = env.reset(seed=seed[episode])\n","    else:\n","      state, info = env.reset()\n","    step = 0\n","    truncated = False\n","    terminated = False\n","    total_rewards_ep = 0\n","\n","    for step in range(max_steps):\n","      # Take the action (index) that have the maximum expected future reward given that state\n","      action = greedy_policy(Q, state)\n","      new_state, reward, terminated, truncated, info = env.step(action)\n","      total_rewards_ep += reward\n","\n","      if terminated or truncated:\n","        break\n","      state = new_state\n","    episode_rewards.append(total_rewards_ep)\n","  mean_reward = np.mean(episode_rewards)\n","  std_reward = np.std(episode_rewards)\n","\n","  return mean_reward, std_reward"]},{"cell_type":"markdown","metadata":{"id":"0jJqjaoAnxUo"},"source":["## Evaluate our Q-Learning agent üìà\n","\n","- Usually, you should have a mean reward of 1.0\n","- The **environment is relatively easy** since the state space is really small (16). What you can try to do is [to replace it with the slippery version](https://gymnasium.farama.org/environments/toy_text/frozen_lake/), which introduces stochasticity, making the environment more complex."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fAgB7s0HEFMm"},"outputs":[],"source":["# Evaluate our Agent\n","mean_reward, std_reward =   # Your code here\n","print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"]},{"cell_type":"markdown","source":["#### Solution"],"metadata":{"id":"SrdzAx3unt7G"}},{"cell_type":"code","source":["# Evaluate our Agent\n","mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n","print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"],"metadata":{"id":"gfYiCbO4nkPy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QZ5LrR-joIHD"},"source":["## Visualize our Results\n","\n","In order to visualize our agent's policy in action use the function `record_video` below.\n","\n","Run the cells to see the results in a GIF."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qo57HBn3W74O"},"outputs":[],"source":["def record_video(env, Qtable, out_directory, fps=1):\n","  \"\"\"\n","  Generate a replay video of the agent\n","  :param env\n","  :param Qtable: Qtable of our agent\n","  :param out_directory\n","  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n","  \"\"\"\n","  images = []\n","  terminated = False\n","  truncated = False\n","  state, info = env.reset(seed=random.randint(0,500))\n","  img = env.render()\n","  images.append(img)\n","  while not terminated or truncated:\n","    # Take the action (index) that have the maximum expected future reward given that state\n","    action = np.argmax(Qtable[state][:])\n","    state, reward, terminated, truncated, info = env.step(action) # We directly put next_state = state for recording logic\n","    img = env.render()\n","    images.append(img)\n","  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"]},{"cell_type":"code","source":["record_video(env, Qtable_frozenlake, '/tmp/result_frozen.gif')\n","Image(open('/tmp/result_frozen.gif','rb').read())"],"metadata":{"id":"G5QUn0iFQvXW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E2875IGsprzq"},"source":["Congrats ü•≥ you've just implemented from scratch and trained your first Reinforcement Learning agent.\n","However, FrozenLake-v1 no_slippery is very simple environment, let's try a harder one üî•."]},{"cell_type":"markdown","metadata":{"id":"18lN8Bz7yvLt"},"source":["# Part 2: Taxi-v3 üöñ\n","\n","## Create and understand the [Taxi-v3 üöï](https://gymnasium.farama.org/environments/toy_text/taxi/) environment\n","---\n","\n","üí° A good habit when you start to use an environment is to check its documentation\n","\n","üëâ https://gymnasium.farama.org/environments/toy_text/taxi/\n","\n","‚ùóÔ∏è Understand how the states are defined in Taxi-v3.\n","\n","---\n","\n","In `Taxi-v3` üöï, there are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue).\n","\n","When the episode starts, **the taxi starts off at a random square** and the passenger is at a random location. The taxi drives to the passenger‚Äôs location, **picks up the passenger**, drives to the passenger‚Äôs destination (another one of the four specified locations), and then **drops off the passenger**. Once the passenger is dropped off, the episode ends.\n","\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi.png\" alt=\"Taxi\">\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gL0wpeO8gpej"},"outputs":[],"source":["env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")"]},{"cell_type":"markdown","metadata":{"id":"gBOaXgtsrmtT"},"source":["There are **500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger** (including the case when the passenger is in the taxi), and **4 destination locations.**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_TPNaGSZrgqA"},"outputs":[],"source":["state_space = env.observation_space.n\n","print(\"There are \", state_space, \" possible states\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CdeeZuokrhit"},"outputs":[],"source":["action_space = env.action_space.n\n","print(\"There are \", action_space, \" possible actions\")"]},{"cell_type":"markdown","metadata":{"id":"R1r50Advrh5Q"},"source":["The action space (the set of possible actions the agent can take) is discrete with **6 actions available üéÆ**:\n","\n","- 0: move south\n","- 1: move north\n","- 2: move east\n","- 3: move west\n","- 4: pickup passenger\n","- 5: drop off passenger\n","\n","Reward function üí∞:\n","\n","- -1 per step unless other reward is triggered.\n","- +20 delivering passenger.\n","- -10 executing ‚Äúpickup‚Äù and ‚Äúdrop-off‚Äù actions illegally."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"US3yDXnEtY9I"},"outputs":[],"source":["# Create our Q table with state_size rows and action_size columns (500x6)\n","Qtable_taxi = initialize_q_table(state_space, action_space)\n","print(Qtable_taxi)\n","print(\"Q-table shape: \", Qtable_taxi .shape)"]},{"cell_type":"markdown","metadata":{"id":"gUMKPH0_LJyH"},"source":["## Define the hyperparameters ‚öôÔ∏è\n","\n","‚ö†Ô∏è DO NOT MODIFY EVAL_SEED: the eval_seed array **allows us to evaluate your agent with the same taxi starting positions for every classmate**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AB6n__hhg7YS"},"outputs":[],"source":["# Training parameters\n","n_training_episodes = 25000   # Total training episodes\n","learning_rate = 0.2           # Learning rate\n","\n","# Evaluation parameters\n","n_eval_episodes = 100        # Total number of test episodes\n","\n","# DO NOT MODIFY EVAL_SEED\n","eval_seed = [16,54,165,177,191,191,120,80,149,178,48,38,6,125,174,73,50,172,100,148,146,6,25,40,68,148,49,167,9,97,164,176,61,7,54,55,\n"," 161,131,184,51,170,12,120,113,95,126,51,98,36,135,54,82,45,95,89,59,95,124,9,113,58,85,51,134,121,169,105,21,30,11,50,65,12,43,82,145,152,97,106,55,31,85,38,\n"," 112,102,168,123,97,21,83,158,26,80,63,5,81,32,11,28,148] # Evaluation seed, this ensures that all classmates agents are trained on the same taxi starting position\n","                                                          # Each seed has a specific starting state\n","\n","# Environment parameters\n","env_id = \"Taxi-v3\"           # Name of the environment, do not modify\n","max_steps = 99               # Max steps per episode\n","gamma = 0.5                 # Discounting rate\n","\n","# Exploration parameters\n","max_epsilon = 1.0             # Exploration probability at start\n","min_epsilon = 0.05           # Minimum exploration probability\n","decay_rate = 0.005            # Exponential decay rate for exploration prob\n"]},{"cell_type":"markdown","metadata":{"id":"1TMORo1VLTsX"},"source":["## Train our Q-Learning agent üèÉ\n","\n","Train your Q-learning agent to obtain a policy œÄ."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WwP3Y2z2eS-K"},"outputs":[],"source":["Qtable_taxi =\n","print(Qtable_taxi)"]},{"cell_type":"markdown","source":["Evaluate your learned policy œÄ.\n"],"metadata":{"id":"5nV8oxfliFDd"}},{"cell_type":"code","source":["mean_reward, std_reward =\n","print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"],"metadata":{"id":"iIsO5A-aiDpE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Visualize the agent controlling a taxi.\n","Different scenarios are shown for each execution of this cell. But the agent always follows the same policy defined by the Q-table."],"metadata":{"id":"PHoPZc6Xe5vQ"}},{"cell_type":"code","source":["record_video(env, Qtable_taxi, '/tmp/result_taxi.gif')\n","Image(open('/tmp/result_taxi.gif','rb').read())"],"metadata":{"id":"Qc1mJQ29e46Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Solution\n"],"metadata":{"id":"zwOwyZRloMb7"}},{"cell_type":"code","source":["Qtable_taxi = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_taxi, gamma, learning_rate)\n","print(Qtable_taxi)\n","\n","mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_taxi, eval_seed)\n","print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")\n","\n","\n","\n","# There are many parameters that will provide optimal solutions.\n","# Here is one solution\n","learning_rate = 0.7           # Learning rate\n","gamma = 0.95                 # Discounting rate"],"metadata":{"id":"4pAZ0s-LoO2h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Cogratulations ü•≥\n","\n","You did it. You learned how to train an agent from scratch.\n","\n","Now, to make your agent better and get higher scores, try adjusting the (hyper-)parameters above and train your agents again.\n","\n","Can you get a mean reward higher than 7.0 ?\n","\n","Good luck!\n","\n","---\n","  \n","    \n","\n","If you were able to get a higher reward than 7.0, then have a look at the FrozenLake environment again.\n","\n","- Does your agent still perform well when you activate the *slippery* mode? This mode makes the state transitions non-deterministic. This means due to the ice's slipperyness there is a chance of you going a different direction than you chose.\n","- Are you able to train your agent in an environment that is larger than 4x4? Maybe 8x8?"],"metadata":{"id":"on2RgpItfKMv"}}],"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"https://github.com/ai-fm/q-learning-notbeook/blob/main/Q_Learning_exercise.ipynb","timestamp":1718884379540}],"collapsed_sections":["Ji_UrI5l2zzn","67OdoKL63eDD","B2_-8b8z5k54","8R5ej1fS4P2V"]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}